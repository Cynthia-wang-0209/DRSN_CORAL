{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58b8a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, decomposition\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "#import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from pandas import Series\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import Conv1d, MaxPool1d, Flatten, Linear, LeakyReLU, ReLU, Sigmoid, Softmax, BatchNorm1d, Sequential, AdaptiveAvgPool1d, Dropout\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"1\"\n",
    "torch.cuda.set_device(2)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "613064d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CORAL_loss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CORAL_loss, self).__init__()\n",
    "\t\t\n",
    "\tdef forward(self, source, target):\n",
    "\t\td = source.data.shape[1]\n",
    "\t\t\n",
    "\t\t# source covariance\n",
    "\t\txm = torch.mean(source, 0, keepdim=True) - source\n",
    "\t\txc = xm.t() @ xm\n",
    "\t\t\n",
    "\t\t# target covariance\n",
    "\t\txmt = torch.mean(target, 0, keepdim=True) - target\n",
    "\t\txct = xmt.t() @ xmt\n",
    "\t\t\n",
    "\t\t# frobenius norm between source and target\n",
    "\t\tloss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n",
    "\t\tloss = loss/(4 * d * d)\n",
    "\t\t\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8d03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                                However, if the field size_average is set to False, the losses are\n",
    "                                instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, class_num=2, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs, dim=1)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c105b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rsbu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rsbu, self).__init__()\n",
    "        self.model0 = Sequential(\n",
    "            Conv1d(1, 8, kernel_size=4, padding=1, stride=2),\n",
    "            LeakyReLU(),\n",
    "            Conv1d(8, 16, kernel_size=3, padding=1, stride=1),\n",
    "            LeakyReLU(),\n",
    "            Conv1d(16, 32, kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "        self.model1 = Sequential(\n",
    "            #BatchNorm1d(32),\n",
    "            LeakyReLU(),\n",
    "            Conv1d(32, 64, kernel_size=3, padding=1, stride=1),\n",
    "            #BatchNorm1d(64),\n",
    "            LeakyReLU(),\n",
    "            Conv1d(64, 32, kernel_size=3, padding=1, stride=1)\n",
    "        )     \n",
    "        \n",
    "        self.model2 = Sequential(\n",
    "            #Flatten(),\n",
    "            Linear(32, 32),\n",
    "            #BatchNorm1d(32),\n",
    "            LeakyReLU(),\n",
    "            Linear(32, 32),\n",
    "            Sigmoid()     \n",
    "        )\n",
    "        \n",
    "        self.model3 = Sequential(\n",
    "            #BatchNorm1d(32),\n",
    "            LeakyReLU(),\n",
    "            Flatten(),\n",
    "            Linear(8*32,2),\n",
    "            LeakyReLU(),\n",
    "            #Dropout(0.5),               \n",
    "        )\n",
    "        \n",
    "        self.class_layer = Sequential(\n",
    "            #Linear(16,2),\n",
    "            Softmax(dim=1)\n",
    "        )\n",
    "        self.gap = AdaptiveAvgPool1d(1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0.1)\n",
    "            elif isinstance(m, (Linear, BatchNorm1d)):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "                nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "             \n",
    "    def forward(self, xs, xt):\n",
    "        xs_bf_thre = self.model0(xs)\n",
    "        xt_bf_thre = self.model0(xt)\n",
    "        xs_thre = self.model1(xs_bf_thre)\n",
    "        xt_thre = self.model1(xt_bf_thre)\n",
    "        xs_abs = torch.abs(xs_thre)\n",
    "        xt_abs = torch.abs(xt_thre)\n",
    "        xs_out = self.gap(xs_abs) \n",
    "        xt_out = self.gap(xt_abs)\n",
    "        xs_out = xs_out.reshape(64, 32)\n",
    "        xt_out = xt_out.reshape(64, 32)\n",
    "        xs_a = self.model2(xs_out)\n",
    "        xt_a = self.model2(xt_out)\n",
    "        xs_a = xs_a.reshape(64, 32, 1)\n",
    "        xt_a = xt_a.reshape(64, 32, 1)\n",
    "        xs_out = xs_out.reshape(64, 32, 1)\n",
    "        xt_out = xt_out.reshape(64, 32, 1)\n",
    "        xs_limit = xs_a * xs_out\n",
    "        xt_limit = xt_a * xt_out\n",
    "        \n",
    "        #soft thresholding\n",
    "        xs_sub = xs_abs - xs_limit\n",
    "        xt_sub = xt_abs - xt_limit\n",
    "        xs_zeros = xs_sub - xs_sub\n",
    "        xt_zeros = xt_sub - xt_sub\n",
    "        xs_n_sub = torch.max(xs_sub, xs_zeros)\n",
    "        xt_n_sub = torch.max(xt_sub, xt_zeros)\n",
    "        xs_soft_thres = xs_n_sub * (torch.sign(xs_thre))\n",
    "        xt_soft_thres = xt_n_sub * (torch.sign(xt_thre))\n",
    "        xs_shortcut = xs_soft_thres + xs_bf_thre\n",
    "        xt_shortcut = xt_soft_thres + xt_bf_thre\n",
    "        #print(soft_thres.size())\n",
    "        xs_features = self.model3(xs_shortcut)\n",
    "        xt_features = self.model3(xt_shortcut)\n",
    "        \n",
    "        coral_loss_model = CORAL_loss()\n",
    "        coral_loss = coral_loss_model(xs_features, xt_features)\n",
    "        output = self.class_layer(xs_features)\n",
    "        return output, coral_loss\n",
    "    \n",
    "    def predict(self, xt):\n",
    "        xt_bf_thre = self.model0(xt)\n",
    "        xt_thre = self.model1(xt_bf_thre)\n",
    "        xt_abs = torch.abs(xt_thre)\n",
    "        xt_out = self.gap(xt_abs)\n",
    "        xt_out = xt_out.reshape(64, 32)\n",
    "        xt_a = self.model2(xt_out)\n",
    "        xt_a = xt_a.reshape(64, 32, 1)\n",
    "        xt_out = xt_out.reshape(64, 32, 1)\n",
    "        xt_limit = xt_a * xt_out\n",
    "        \n",
    "        #soft thresholding\n",
    "        xt_sub = xt_abs - xt_limit\n",
    "        xt_zeros = xt_sub - xt_sub\n",
    "        xt_n_sub = torch.max(xt_sub, xt_zeros)\n",
    "        xt_soft_thres = xt_n_sub * (torch.sign(xt_thre))\n",
    "        xt_shortcut = xt_soft_thres + xt_bf_thre\n",
    "        #print(soft_thres.size())\n",
    "        \n",
    "        \n",
    "        xt_features = self.model3(xt_shortcut)\n",
    "        output = self.class_layer(xt_features)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84f04789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据所在路径\n",
    "def obtain_csv(dir_name, purpose):\n",
    "    data_path = []\n",
    "    dir_path = os.path.join(dir_name, purpose)\n",
    "    file_lists = os.listdir(dir_path)\n",
    "    file_lists.sort()\n",
    "    if purpose == 'train':\n",
    "        for file_list in file_lists:\n",
    "            file_path = os.path.join(dir_path, file_list)\n",
    "            data_path.append(file_path)\n",
    "    else:\n",
    "        file_list = 'data_test.csv'\n",
    "        data_path = os.path.join(dir_path, file_list)\n",
    "    return data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49a8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取经过pca处理后的数据与标签\n",
    "def load_data_KX09(data_path):\n",
    "    data = pd.read_csv(data_path,encoding='gbk')\n",
    "    data = data.drop(['卫星时间'], axis=1)\n",
    "    data = data.dropna(axis=1)\n",
    "    columns = data.columns\n",
    "    drop_list = []\n",
    "    for f in list(columns):\n",
    "        try:\n",
    "            data[f] = data[f].astype(float)\n",
    "        except:\n",
    "            pass\n",
    "        if len(list(pd.unique(data[f]))) == 1:\n",
    "            drop_list.append(f)\n",
    "    data = data.drop(drop_list, axis=1)\n",
    "    data = np.array(data)\n",
    "    y = data[:,-1].astype(int)\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    data = np.delete(data, -1, axis=1)\n",
    "    #归一化+标准化\n",
    "    scalar = preprocessing.MinMaxScaler()\n",
    "    data = scalar.fit_transform(data)\n",
    "    data = preprocessing.scale(data)\n",
    "    #pca\n",
    "    n_components = 16\n",
    "    pca = decomposition.PCA(n_components=16)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    #pca后的归一化\n",
    "    pca_data = scalar.fit_transform(pca_data)\n",
    "    #pca_data = preprocessing.scale(pca_data)\n",
    "    data_label = np.concatenate((pca_data, y[:,None]), axis=1)\n",
    "    data_size = len(data_label)\n",
    "    return data_label, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64998b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取量子卫星0087数据\n",
    "def load_data_0087(data_path):\n",
    "    data = pd.read_csv(data_path, encoding='gbk')\n",
    "    data = data.drop(index=0)\n",
    "    data = data.dropna(axis=1)\n",
    "    y = data['errtype3'].astype(int)\n",
    "    y = np.array(y)\n",
    "    data = data.drop(['TMKZ001', 'TMKZ002','errtype1', 'errtype2', 'errtype3', \n",
    "                      'errtype4', 'errtype5','errtype6', 'errtype7'], axis=1)\n",
    "\n",
    "    # 去掉只有一个值的列\n",
    "    columns = data.columns\n",
    "    drop_list = []\n",
    "    for f in list(columns):\n",
    "        try:\n",
    "            data[f] = data[f].astype(float)\n",
    "        except:\n",
    "            pass\n",
    "        if len(list(pd.unique(data[f]))) == 1:\n",
    "            drop_list.append(f)\n",
    "    data = data.drop(drop_list, axis=1)\n",
    "\n",
    "    # 转onehot\n",
    "    object_list = []\n",
    "    for f in list(data.columns):\n",
    "        if data[f].dtype == 'object':\n",
    "            object_list.append(f)\n",
    "    for f in object_list:\n",
    "        data_dummies = pd.get_dummies(data[f])\n",
    "        data = data.drop([f], axis=1)\n",
    "        data = pd.concat([data, data_dummies], axis=1)  \n",
    "    data = np.array(data)\n",
    "    \n",
    "    #归一化+标准化\n",
    "    scalar = preprocessing.MinMaxScaler()\n",
    "    data = scalar.fit_transform(data)\n",
    "    data = preprocessing.scale(data)\n",
    "    #pca\n",
    "    n_components = 16\n",
    "    pca = decomposition.PCA(n_components=16)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    #pca后的归一化\n",
    "    pca_data = scalar.fit_transform(pca_data)\n",
    "    #pca_data = preprocessing.scale(pca_data)\n",
    "    data_label = np.concatenate((pca_data, y[:,None]), axis=1)\n",
    "    data_size = len(data_label)\n",
    "    return data_label, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f481b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(data):\n",
    "    label = data[:,-1].to(torch.int)\n",
    "    data = np.delete(data, -1, axis=1)\n",
    "    data = data.view((-1, 1, 16))\n",
    "    data = data.to(torch.float32)\n",
    "    label = label.to(torch.long)\n",
    "    device = torch.device('cuda:2')\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_SNE(data):\n",
    "    X_tsne = TSNE(n_components=2, random_state=10, learning_rate='auto', init='random').fit_transform(data)\n",
    "    data_tsne = np.concatenate((X_tsne, label[:,None]), axis=1)\n",
    "    data_tsne = pd.DataFrame(data_tsne)\n",
    "    return data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsne_s.columns = ['dim1_s', 'dim2_s', 'label_s']\n",
    "data_tsne_t.columns = ['dim1_t', 'dim2_t', 'label_t']\n",
    "group_s = data_tsne_s.groupby('label_s')\n",
    "group_t = data_tsne_t.groupby('label_t')\n",
    "fig = plt.figure(figsize=(5,5), facecolor='white')\n",
    "for key, value in group_s:\n",
    "    d = value\n",
    "    #plt.style.use(\"classic\")\n",
    "    if key == 0:\n",
    "        plt.plot(d['dim1_s'], d['dim2_s'], linestyle='',color='k', marker='.', label='A1 anomaly')\n",
    "        plt.legend(loc='upper right', prop=font)\n",
    "    else:\n",
    "        plt.plot(d['dim1_s'], d['dim2_s'], linestyle='',color='k', marker='3', label='B1 anomaly')\n",
    "        plt.legend(loc='upper left', prop=font)\n",
    "        \n",
    "for key, value in group_t:\n",
    "    d = value\n",
    "    #plt.style.use(\"classic\")\n",
    "    if key == 0:\n",
    "        plt.plot(d['dim1_t'], d['dim2_t'], linestyle='',color='k', marker='x', label='A1 normal', markersize=4)\n",
    "        plt.legend(loc='upper right', prop=font)\n",
    "    else:\n",
    "        plt.plot(d['dim1_t'], d['dim2_t'], linestyle='',color='k', marker='d', label='B1 normal', markersize=4)\n",
    "        plt.legend(loc='upper left', prop=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f1a5161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wal",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
